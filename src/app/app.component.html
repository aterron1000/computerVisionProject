<mat-card>
  <mat-card-title>Abstract</mat-card-title>
  <mat-divider></mat-divider>
  <mat-card-content>
    <p>
      Traditional Human-Computer Interaction (HCI) relies heavily on 
      physical devices like mouse and keyboard, which limits accessibility, 
      adaptability and efficiency in modern applications. There is a growing 
      need for contactless and intuitive interaction to improve accessibility 
      and hygiene in environments where it is necessary, such as hospitals 
      and smart environments.
    </p>
    <p>
      This project demonstrates the development of a hand gesture controller 
      system using computer vision techniques. The system utilizes OpenCV for 
      real-time gesture detection and MediaPipe for hand tracking and landmark 
      identification. Computer vision in combination with MediaPipe was used to 
      track landmarks of the hand for mouse gestures, and these landmarks were 
      used to determine the region of interest for a convolutional neural network 
      to classify hand gestures. The controller underwent extensive testing showing 
      promising performance in gesture detection accuracy, responsiveness, and 
      robustness across varying environmental conditions.

    </p>

  </mat-card-content>
</mat-card>

<mat-card>
  <mat-card-title>Introduction</mat-card-title>
  <mat-divider></mat-divider>
  <mat-card-content>
    <p>The integration of hand gesture control systems has garnered significant 
      attention in human-computer interaction research due to its potential to 
      enhance user experience and accessibility. Traditional input devices such 
      as keyboards and mice present limitations in terms of natural interaction, 
      accessibility for users with disabilities, and hygiene concerns in shared or 
      medical environments.
  </p>
  <p>
    This project, named the KillJoy Automated System, addresses these challenges 
    by developing a comprehensive hand gesture controller that enables users to 
    interface with computer systems in a natural, intuitive, and efficient manner. 
    The system aims to overcome traditional input device limitations by incorporating 
    advanced computer vision techniques for hand gesture recognition.
  </p>
  <p>
    The fundamental motivation for this project stems from the increasing demand 
    for novel interaction modalities that can improve user experience and productivity 
    across various domains, including gaming, virtual reality, smart environments, 
    and assistive technologies. By leveraging hand gestures, the system provides users 
    with greater flexibility and freedom in controlling and navigating digital interfaces.
  </p>
  <h1>Objectives</h1>
  <ul>
    <li>Designing a gesture recognition system utilizing computer vision techniques</li>
    <li>Creating a controller system for hand gesture recognition</li>
    <li>Optimizing performance for real-time responsiveness and low latency</li>
    <li>Ensuring accessibility and usability for users with diverse needs and abilities</li>
  </ul>
  </mat-card-content>
</mat-card>

<mat-card>
  <mat-card-title>Related Work / Background</mat-card-title>
  <mat-divider></mat-divider>
  <mat-card-content>
    <p>
      Gesture recognition has evolved from older glove-based and fingertip-color-tracking 
      approaches, which suffered from accuracy limitations and lack of comfort.
    </p>
    <p>Modern techniques now leverage:</p>
    <ul>
      <li>Computer Vision Advancements</li>
      <li>Deep Learning Architectures (CNN)</li>
      <li>Hand Landmark Detection (MediaPipe)</li>
    </ul>
    <p>These have dramatically improved robustness in lighting variations, 
      motion, and real-time performance.
    </p>
    <p>
      OpenCV provides essential image-processing tools, while MediaPipe delivers 
      highly reliable 21-point hand landmark tracking in real-time.
    </p>
  </mat-card-content>
</mat-card>

<mat-card>
  <mat-card-title>Approach</mat-card-title>
  <mat-divider></mat-divider>
  <mat-card-content>
    <h1 class="bold">Landmarks</h1>
    <p>System Architecture</p>
    <p class="bold">1) Image Sensor Module</p>
    <ul>
      <li>Captures real-time frames</li>
      <li>Adjusts camera resolution and FPS</li>
      <li>Sends frames for processing</li>
    </ul>
    <p class="bold">2) Detection Module</p>
    <ul>
      <li>Uses MediaPipe for hand tracking</li>
      <li>Extracts landmarks</li>
      <li>Interprets gesture patterns</li>
    </ul>
    <p class="bold">3) Connection Module</p>
    <ul>
      <li>Manages data communication</li>
      <li>Optimizes latency</li>
      <li>Handles exceptions</li>
    </ul>
    <p class="bold">4) Interface Module</p>
    <ul>
      <li>Maps gestures to system actions</li>
      <li>Integrates with applications (PowerPoint, media players, etc)</li>
      <li>Offers customization</li>
    </ul>
    <h1>Images</h1>
    <img src="../assets/Images/Landmarks.png" width="400">
    <img src="../assets/Images/Landmarks2.png" width="150">
    <mat-divider></mat-divider>
    <h1 class="bold">CNN</h1>
    <h1>Dataset</h1>
    <p>
      A new dataset was created containing different gestures with the hand on 
      one position, to avoid overfitting the data was captured in different 
      scenarios such as:
    </p>
    <ul>
      <li>Different levels of illumination</li>
      <li>Different solid colored backgrounds</li>
      <li>Natural backgrounds containing trees</li>
      <li>Modern environments with furniture as background</li>
    </ul>
    <p>
      A series of videos were captured with the hand doing the gesture intended with the
      varied scenarios as described before. Based on these videos a new dataset was created
      containing 7 different gestures, each class containing 7,000 images.
    </p>
    <p>
      To facilitate the process of capturing the images from each frame, MediaPipe was used to take the 
      landmarks of the hand and crop out everything else. This allowed to keep an image of the hand only
      doing the intended gesture, this image was resized to 3 x 224 x 224 so it was consistent across
      the entire dataset.
    </p>
    <h1>Pre-Processing and Training</h1>
    <p>Three different CNNs were trained with their RGB and Grayscale variants 
      with the previous dataset to determine which one has the most precision for gesture classification:
    </p>
    <ul>
      <li>VGG 16 (3 x 227 x 227)</li>
      <li>VGG 16 - Grayscale (1 x 227 x 227)</li>
      <li>AlexNet (3 x 227 x 227)</li>
      <li>AlexNet - Grayscale (1 x 227 x 227)</li>
      <li>ResNet (3 x 224 x 224)</li>
      <li>ResNet - Grayscale (1 x 224 x 224)</li>
    </ul>
    <p>
      To further avoid overfitting during training and with the objective
      to improve the accuracy of the model the data was augmented during pre-processing with
      the next modifications:
    </p>
    <ul>
      <li>Resize (To the model input)</li>
      <li>Random Auto Contrast</li>
      <li>Random rotation (Up to 180 degrees)</li>
      <li>Random Sharpness</li>
      <li>Random Grayscale</li>
      <li>Random Perspective</li>
      <li>Normalize</li>
    </ul>
    <p>The next images show a sample of the augmented images in RGB and Grayscale respectively</p>
    <img src="../assets/Images/datasetSample.png" width="400">
    <img src="../assets/Images/datasetSample2.png" width="400">
    
  </mat-card-content>
</mat-card>
<mat-card>
  <mat-card-title>Initial observations & Implementation</mat-card-title>
  <mat-divider></mat-divider>
  <mat-card-content>
    <p>A Python program was developed with the objective to become a new device 
      input for the Windows operating system that would act as mouse and keyboard, 
      this program implements both approaches, each one taking advantage of their 
      strengths:
    </p>
    <h1 class="bold">Landmarks Approach</h1>
    <p>Early testing revealed several key factors that influenced system performance:</p>
    <ul>
      <li>Lighting variations affected hand visibility and contour detection.</li>
      <li>Complex backgrounds caused noise and reduced accuracy.</li>
      <li>Differences in hand size, shape, and orientation required strong generalization.</li>
      <li>Camera distance and angle impacted gesture clarity.</li>
      <li>Similar-looking gestures created ambiguity without landmark-based detection.</li>
    </ul>
    <p>
      These observations guided the choice of MediaPipe for stable hand tracking and the need for ROI 
      extraction and data augmentation.
    </p>
    <h1>Implementation</h1>
    <p>Development progressed using PyCharm with modules built step-by-step:</p>
    <ul>
      <li>Real-time video capture for smooth frame streaming.</li>
      <li>MediaPipe hand tracking to extract 21 high-accuracy landmarks.</li>
      <li>ROI extraction to isolate the hand for CNN processing.</li>
      <li>Hybrid gesture logic combining landmark-based rules and CNN classification.</li>
      <li>Integration layer to map gestures to actions such as slide control, media 
        commands, and pointer movement.</li>
    </ul>
    <p>This modular approach ensured efficient, real-time, and reliable gesture-based interaction.</p>

    <h1 class="bold">CNN Approach</h1>
    <p>
      Using a CNN gives the advantage for unique gestures to be detected on the webcam, 
      during initial testing and given ideal circumstances like lighting and plain background, 
      most models could detect the gestures with good accuracy.
    </p>
    <p>
      AlexNet and ResNet struggled with the recognition of gestures once the hand was on 
      a background that had low contrast against the hand, sometimes not recognizing the 
      gesture if it was too complex, but one advantage is that they were noticeably 
      faster to execute.
    </p>
    <p class="bold">[VIDEO EXAMPLE]</p>
    <p>On the other side VGG16 found the best results even if the hand was found in 
      complicated backgrounds with low contrast or with multiple objects behind the hand, 
      indicating that the model was less overfit and instead was detecting the features of 
      the gesture correctly, at the expense of using more computer resources.
    </p>
    <p class="bold">[VIDEO EXAMPLE]</p>
    <p>
      The final implementation of the program used the landmarks, since it was found to 
      be ideal to track the mouse and detect the gestures for mice actions as it was more 
      efficient to execute.
    </p>
    <p>
      Meanwhile the CNN approach was used for more particular actions that required 
      more computer resources to detect among the 7 different trained gestures from 
      the model, the ideal model selected was VGG16 - RGB as it was more accurate in 
      environments where the hand was on complicated scenarios like low contrast or 
      with objects and furniture behind.
    </p>
    <p>
      To not run the CNN model and the Landmarks model at the same time with the 
      objective to save resources, an approach of using the hands position was used 
      where if the hand is facing forward, the landmarks model is used while if the 
      hand is facing backwards, the CNN model is used.
    </p>
  </mat-card-content>
</mat-card>

<mat-card>
  <mat-card-title>Experiments & Results</mat-card-title>
  <mat-divider></mat-divider>
  <mat-card-content>
    <h1 class="bold">Training Results</h1>
    <h1>AlexNet</h1>
    <h3>Confusion Matrix</h3>
    <img src="../assets/Images/AlexNet-CF.png" width="400">
    <img src="../assets/Images/AlexNet-Train.png" width="400">

    <h1>AlexNet - Gray</h1>
    <h3>Confusion Matrix</h3>
    <img src="../assets/Images/AlexNetGray-CF.png" width="400">
    <img src="../assets/Images/AlexNet-Gray-Train.png" width="400">
    <mat-divider></mat-divider>

    <h1>VGG16</h1>
    <h3>Confusion Matrix</h3>
    <img src="../assets/Images/VGG16-CF.png" width="400">
    <img src="../assets/Images/VGG16-Train.png" width="400">

    <h1>VGG16 - Gray</h1>
    <h3>Confusion Matrix</h3>
    <img src="../assets/Images/VGG16Gray-CF.png" width="400">
    <img src="../assets/Images/VGG16-Gray-Train.png" width="400">
    <mat-divider></mat-divider>

    <h1>ResNet</h1>
    <h3>Confusion Matrix</h3>
    <img src="../assets/Images/ResNet-CF.png" width="400">
    <img src="../assets/Images/ResNet-Train.png" width="400">

    <h1>ResNet - Gray</h1>
    <h3>Confusion Matrix</h3>
    <img src="../assets/Images/ResNetGray-CF.png" width="400">
    <img src="../assets/Images/ResNet-Gray-Train.png" width="400">
  </mat-card-content>
</mat-card>

<mat-card>
  <mat-card-title>Discussion & Limitations</mat-card-title>
  <mat-divider></mat-divider>
  <mat-card-content>
    <h2>Discussion</h2>
    <p>The implemented gesture recognition system performed effectively in real time, 
      with MediaPipe providing accurate landmark tracking and the CNN models delivering 
      strong classification results. The hybrid design—using landmarks for continuous 
      gestures and CNNs for discrete gestures—helped achieve both speed and accuracy. 
      User testing showed the system to be intuitive, responsive, and suitable for 
      applications like presentation control, media interaction, and general computer 
      navigation.</p>
      <h2>Limitations</h2>
      <p>
        Despite the strong performance for Handtracking and CNN, several limitations were identified:
      </p>
      <ul>
        <li>Low Light Environments reduce tracking accuracy</li>
        <li>Complex or cluttered backgrounds can introduce noise in detection</li>
        <li>Limited gesture vocabulary restricts functionality without more training data</li>
        <li>Hardware dependency may limit performance on low-power devices</li>
        <li>User adaptation is required for consistent hand positioning and angles.</li>
      </ul>
      <h2>Future Work</h2>
      <p>There are some steps that can be taken to enhance the system further:</p>
      <ul>
        <li>Create larger datasets, including dynamic gestures and two hand interactions</li>
        <li>Increase flexibility like the personalization of gesture learning for different users</li>
        <li>Gather more data with gestures on environments that are cluttered and with low light</li>
        <li>Design a more efficient CNN model for gesture recognition so that it can be
          implemented in low powered devices</li>
        <li>Implement the program in different platforms like mobile for wider accessibility.</li>
      </ul>
  </mat-card-content>
</mat-card>

<mat-card>
  <mat-card-title>Conclusion</mat-card-title>
  <mat-divider></mat-divider>
  <mat-card-content>
    <p>
      This project successfully developed and implemented a comprehensive hand gesture 
      controller system that enables natural and intuitive interaction with computer 
      systems. By integrating OpenCV for real-time video processing and MediaPipe for 
      accurate hand landmark detection, the system provides users with a touchless and 
      efficient interaction method that overcomes many limitations of traditional input 
      devices.
    </p>
    <p>
      The incremental development approach allowed for continuous refinement based on 
      user testing and performance evaluation. Experiments conducted across multiple 
      CNN architectures helped identify optimal configurations that balance accuracy, 
      speed, and computational efficiency, enabling real-time gesture recognition 
      suitable for practical, everyday applications.
    </p>
    <p>
      This work contributes to the broader field of human-computer interaction by 
      demonstrating a robust and reliable gesture-based interface. With potential 
      applications in assistive technologies, smart environments, virtual reality, 
      and interactive systems, the project represents a meaningful step toward more 
      accessible, hygienic, and intuitive digital interfaces.
    </p>
  </mat-card-content>
</mat-card>